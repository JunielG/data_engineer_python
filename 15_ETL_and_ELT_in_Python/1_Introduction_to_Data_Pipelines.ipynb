{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae912458-d0b7-4be8-abc8-984040c193fe",
   "metadata": {},
   "source": [
    "## Introduction to ETL and ELT Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825d6c94-9257-47ee-a439-5ebb8ce0b4bb",
   "metadata": {},
   "source": [
    "### Extract, transform, load (ETL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41ad54-4394-4ec9-9060-9b08443dbc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(data_frame, target_table):\n",
    "    # Some custom-built Python logic to load data to SQL   \n",
    "    data_frame.to_sql(name=target_table, con=POSTGRES_CONNECTION)\n",
    "    print(f\"Loading data to the {target_table} table\")\n",
    "    \n",
    "# Now, run the data pipeline\n",
    "extracted_data = extract(file_name=\"raw_data.csv\")\n",
    "transformed_data = transform(data_frame=extracted_data)\n",
    "load(data_frame=transformed_data, target_table=\"cleaned_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68e7280-acae-4719-9058-7c8cae1ad1cf",
   "metadata": {},
   "source": [
    "### Extract, load, transform (ELT)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c0fcfd-6dc2-40b6-9ed2-f7ac38527f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(source_table, target_table):  \n",
    "    data_warehouse.run_sql(\"\"\"\n",
    "    CREATE TABLE {target_table} AS\n",
    "    SELECT \n",
    "    <field-name>, <field-name>, ...   \n",
    "    FROM {source_table};\n",
    "    \"\"\")\n",
    "    \n",
    "# Similar to ETL pipelines, call the extract, load, and transform functions\n",
    "extracted_data = extract(file_name=\"raw_data.csv\")\n",
    "load(data_frame=extracted_data, table_name=\"raw_data\")\n",
    "transform(source_table=\"raw_data\", target_table=\"cleaned_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5632ad5-c15d-4cb7-b8b9-f095b288af89",
   "metadata": {},
   "source": [
    "### Running an ETL Pipeline\n",
    "Ready to run your first ETL pipeline? Let's get to it!\n",
    "\n",
    "Here, the functions extract(), transform(), and load() have been defined for you. To run this data ETL pipeline, you're going to execute each of these functions. If you're curious, take a peek at what the extract() function looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f89bfe4-b8e1-407e-9a48-e0c0c6520ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_name):\n",
    "    print(f\"Extracting data from {file_name}\")\n",
    "    return pd.read_csv(file_name)\n",
    "\n",
    "# Extract data from the raw_data.csv file\n",
    "extracted_data = extract(file_name=\"raw_data.csv\")\n",
    "\n",
    "# Transform the extracted_data\n",
    "transformed_data = transform(data_frame=extracted_data)\n",
    "\n",
    "# Load the transformed_data to cleaned_data.csv\n",
    "load(data_frame=transformed_data, target_table=\"cleaned_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feb6dd7-6cbb-43fe-bc8f-4c08a308c7eb",
   "metadata": {},
   "source": [
    "### ELT in Action\n",
    "Feeling pretty good about running ETL processes? Well, it's time to give ELT pipelines a try. Like before, the extract(), load(), and transform() functions have been defined for you; all you'll have to worry about is running these functions. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bd9a98-f600-4fa8-9a93-edfccd47acd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from the raw_data.csv file\n",
    "raw_data = extract(file_name=\"raw_data.csv\")\n",
    "\n",
    "# Load the extracted_data to the raw_data table\n",
    "load(data_frame=raw_data, table_name=\"raw_data\")\n",
    "\n",
    "# Transform data in the raw_data table\n",
    "transform(\n",
    "  source_table=\"raw_data\", \n",
    "  target_table=\"cleaned_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70731ba2-2834-4f09-a4c8-995315af4186",
   "metadata": {},
   "source": [
    "## Building ETL and ELT Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa15768-a6b8-4329-b097-9c14286a8f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, by rows\n",
    "data_frame.loc[data_frame[\"name\"] == \"Apparel\", :]\n",
    "\n",
    "# Then, by columns\n",
    "data_frame.loc[:, [\"name\", \"num_firms\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4be3644-1c20-43bb-9a33-c2d126d886a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define extract(), transform(), and load() functions... \n",
    "def transform(data_frame, value):\n",
    "    return data_frame.loc[data_frame[\"name\"] == value, [\"name\", \"num_firms\"]]\n",
    "   \n",
    "    # First, extract data from a .csv \n",
    "    extracted_data = extract(file_name=\"raw_data.csv\")\n",
    "    \n",
    "    # Then, transform the `extracted_data`\n",
    "    transformed_data = transform(data_frame=extracted_data, value=\"Apparel\")\n",
    "    \n",
    "    # Finally, load the `transformed_data`\n",
    "    load(data_frame=transformed_data, file_name=\"cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7f747d-5395-41a2-ab60-4731074a2bcb",
   "metadata": {},
   "source": [
    "### Building an ETL Pipeline\n",
    "Ready to ratchet up the fun? In this exercise, you'll be responsible for building the rest of the load() function before running each step in the ETL process. The extract() and transform() functions have been defined for you. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905a97c9-9915-4946-b68d-8ecd7a66fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(data_frame, file_name):\n",
    "  # Write cleaned_data to a CSV using file_name\n",
    "  data_frame.to_csv(file_name)\n",
    "  print(f\"Successfully loaded data to {file_name}\")\n",
    "\n",
    "extracted_data = extract(file_name=\"raw_data.csv\")\n",
    "\n",
    "# Transform extracted_data using transform() function\n",
    "transformed_data = transform(data_frame=extracted_data)\n",
    "\n",
    "# Load transformed_data to the file transformed_data.csv\n",
    "load(data_frame=transformed_data, file_name=\"transformed_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be9ac72-3b3e-41d8-825d-a50b21a90ee2",
   "metadata": {},
   "source": [
    "### The \"T\" in ELT\n",
    "Let's not forget about ELT! Here, the extract() and load() functions have been defined for you. Now, all that's left is to finish defining the transform() function and run the pipeline. Go get 'em!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c8a14a-a71b-4a90-970f-fe68e918ae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete building the transform() function\n",
    "def transform(source_table, target_table):\n",
    "  data_warehouse.execute(f\"\"\"\n",
    "  CREATE TABLE {target_table} AS\n",
    "      SELECT\n",
    "          CONCAT(\"Product ID: \", product_id),\n",
    "          quantity * price\n",
    "      FROM {source_table};\n",
    "  \"\"\")\n",
    "\n",
    "extracted_data = extract(file_name=\"raw_sales_data.csv\")\n",
    "load(data_frame=extracted_data, table_name=\"total_sales\")\n",
    "\n",
    "# Populate total_sales by transforming raw_sales_data\n",
    "transform(source_table='raw_sales_data', target_table='total_sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5613bb0e-0eeb-4807-bd85-06acc4148bf5",
   "metadata": {},
   "source": [
    "### Extracting, Transforming, and Loading Student Scores Data\n",
    "Alright, it's time to build your own ETL pipeline from scratch. In this exercise, you'll build three functions; extract(), transform(), and load(). Then, you'll use these functions to run your pipeline.\n",
    "\n",
    "The pandas library has been imported as pd. Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a7d5dc-291b-4bab-b066-3a02155efb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_name):\n",
    "  # Read a CSV with a path stored using file_name into memory\n",
    "  return pd.read_csv(file_name)\n",
    "\n",
    "def transform(data_frame):\n",
    "  # Filter the data_frame to only incude a subset of columns\n",
    "  return data_frame.loc[:, [\"industry_name\", \"number_of_firms\"]]\n",
    "\n",
    "def load(data_frame, file_name):\n",
    "  # Write the data_frame to a CSV\n",
    "  data_frame.to_csv(file_name)\n",
    "\n",
    "extracted_data = extract(file_name=\"raw_industry_data.csv\")\n",
    "transformed_data = transform(data_frame=extracted_data)\n",
    "\n",
    "# Pass the transformed_data DataFrame to the load() function\n",
    "load(data_frame=transformed_data, file_name=\"number_of_firms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d67f088-4db4-4fa9-a6ba-874350ede7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
