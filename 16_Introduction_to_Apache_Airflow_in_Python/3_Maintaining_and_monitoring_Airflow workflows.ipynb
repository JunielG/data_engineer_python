{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f224a0b1-b852-4ee4-87ae-925cbc15a05b",
   "metadata": {},
   "source": [
    "## Airflow sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b87ca9-4d08-4fa9-8752-f6659ccb0c19",
   "metadata": {},
   "source": [
    "#### Sensor details \n",
    "Derived from airflow.sensors.base_sensor_operator\n",
    "- Sensor arguments:\n",
    "- mode - How to check for the condition\n",
    "- mode='poke' - The default, run repeatedly\n",
    "- mode='reschedule' - Give up task slot and try again later\n",
    "- poke_interval - How often to wait between checks\n",
    "- timeout - How long to wait before failing task\n",
    "- Also includes normal operator attributes\n",
    "\n",
    "#### File sensor \n",
    "- Is part of the airflow.sensors library\n",
    "- Checks for the existence of a file at a certain location\n",
    "- Can also check if any files exist within a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7eda25-eec3-4ae0-8518-27800d2035bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.sensors.filesystem import FileSensor \n",
    "file_sensor_task = FileSensor(task_id='file_sense',      \n",
    "                              filepath='salesdata.csv',       \n",
    "                              poke_interval=300,        \n",
    "                              dag=sales_report_dag)\n",
    "\n",
    "init_sales_cleanup >> file_sensor_task >> generate_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db3cf5e-3b56-41c7-858b-fe412011326b",
   "metadata": {},
   "source": [
    "## Airflow executors "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e0b290-844f-4c2b-b336-ec37938518d7",
   "metadata": {},
   "source": [
    "#### What is an executor?\n",
    "- Executors run tasks \n",
    "- Different executors handle running the tasks differently\n",
    "- Example executors:\\\n",
    "  SequentialExecutor\\\n",
    "  LocalExecutor\\\n",
    "  KubernetesExecutor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f795eba0-03ba-453a-b1d1-256bd1b5a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "from datetime import datetime\n",
    "\n",
    "report_dag = DAG(\n",
    "    dag_id = 'execute_report',\n",
    "    schedule_interval = \"0 0 * * *\"\n",
    ")\n",
    "\n",
    "precheck = FileSensor(\n",
    "    task_id='check_for_datafile',\n",
    "    filepath='salesdata_ready.csv',\n",
    "    start_date=datetime(2024,1,20),\n",
    "    mode='reschedule',\n",
    "    dag=report_dag\n",
    ")\n",
    "\n",
    "generate_report_task = BashOperator(\n",
    "    task_id='generate_report',\n",
    "    bash_command='generate_report.sh',\n",
    "    start_date=datetime(2024,1,20),\n",
    "    dag=report_dag\n",
    ")\n",
    "\n",
    "precheck >> generate_report_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d22e1e-6671-4996-8b12-3cc62d958223",
   "metadata": {},
   "source": [
    "## Debugging and troubleshooting in Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fa98e4-6877-4e45-9978-1f721f850e13",
   "metadata": {},
   "source": [
    "#### Two quick methods: \n",
    "- Run airflow dags list-import-errors\n",
    "- Run python3 <dagfile.py>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24214403-af2e-42ae-b71b-2c6ae451707d",
   "metadata": {},
   "source": [
    "## SLAs and reporting in Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8e9560-ba5a-48af-acf5-02dd48ec065f",
   "metadata": {},
   "source": [
    "#### Defining SLAs \n",
    "- Using the 'sla' argument on the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd447d-870e-40d2-893d-eb39930da480",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1 = BashOperator(task_id='sla_task',        \n",
    "                     bash_command='runcode.sh',     \n",
    "                     sla=timedelta(seconds=30),   \n",
    "                     dag=dag)\n",
    "\n",
    "# On the default_args dictionary\n",
    "default_args={'sla': timedelta(minutes=20),\n",
    "              'start_date': datetime(2023,2,20)\n",
    "             }\n",
    "\n",
    "dag = DAG('sla_dag', default_args=default_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54303502-1f2d-45ca-a41c-066939c52f14",
   "metadata": {},
   "source": [
    "### Defining an SLA\n",
    "You've successfully implemented several Airflow workflows into production, but you don't currently have any method of determining if a workflow takes too long to run. After consulting with your manager and your team, you decide to implement an SLA at the DAG level on a test workflow.\n",
    "\n",
    "All appropriate Airflow libraries have been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e76396d-d25b-44be-b3c6-6c7d15ff66dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the timedelta object\n",
    "from datetime import timedelta\n",
    "\n",
    "# Create the dictionary entry\n",
    "default_args = {\n",
    "  'start_date': datetime(2024, 1, 20),\n",
    "  'sla': timedelta(minutes=30)\n",
    "}\n",
    "\n",
    "# Add to the DAG\n",
    "test_dag = DAG('test_workflow', default_args=default_args, schedule_interval=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131180c1-e575-4812-a7ca-1873da66f33f",
   "metadata": {},
   "source": [
    "### Defining a task SLA\n",
    "After completing the SLA on the entire workflow, you realize you really only need the SLA timing on a specific task instead of the full workflow.\n",
    "\n",
    "The appropriate Airflow libraries are imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dabd55-e8a0-4149-abd6-ffc17cc3053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the timedelta object\n",
    "from datetime import timedelta\n",
    "\n",
    "test_dag = DAG('test_workflow', start_date=datetime(2024,1,20), schedule_interval=None)\n",
    "\n",
    "# Create the task with the SLA\n",
    "task1 = BashOperator(task_id='first_task',\n",
    "                     sla=timedelta(hours=3),\n",
    "                     bash_command='initialize_data.sh',\n",
    "                     dag=test_dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fc609c-d024-44c1-bc97-55dc23ecce33",
   "metadata": {},
   "source": [
    "### Generate and email a report\n",
    "Airflow provides the ability to automate almost any style of workflow. You would like to receive a report from Airflow when tasks complete without requiring constant monitoring of the UI or log files. You decide to use the email functionality within Airflow to provide this message.\n",
    "\n",
    "All the typical Airflow components have been imported for you, and a DAG is already defined as report_dag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b6bb49-043c-4cb7-9791-8c1172a8a593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the email task\n",
    "email_report = EmailOperator(\n",
    "        task_id='email_report',\n",
    "        to='airflow@datacamp.com',\n",
    "        subject='Airflow Monthly Report',\n",
    "        html_content=\"\"\"Attached is your monthly workflow report - please refer to it for more detail\"\"\",\n",
    "        files=['monthly_report.pdf'],\n",
    "        dag=report_dag\n",
    ")\n",
    "\n",
    "# Set the email task to run after the report is generated\n",
    "email_report << generate_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059508cb-e930-4963-8c89-fa9e10896895",
   "metadata": {},
   "source": [
    "### Adding status emails\n",
    "You've worked through most of the Airflow configuration for setting up your workflows, but you realize you're not getting any notifications when DAG runs complete or fail. You'd like to setup email alerting for the success and failure cases, but you want to send it to two addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b2e3e5-b2af-4804-8619-14160bb5e1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "from datetime import datetime\n",
    "\n",
    "default_args={\n",
    "    'email': ['airflowalerts@datacamp.com','airflowadmin@datacamp.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_success': True\n",
    "}\n",
    "\n",
    "report_dag = DAG(\n",
    "    dag_id = 'execute_report',\n",
    "    schedule_interval = \"0 0 * * *\",\n",
    "    default_args=default_args\n",
    ")\n",
    "\n",
    "precheck = FileSensor(\n",
    "    task_id='check_for_datafile',\n",
    "    filepath='salesdata_ready.csv',\n",
    "    start_date=datetime(2023,2,20),\n",
    "    mode='reschedule',\n",
    "    dag=report_dag)\n",
    "\n",
    "generate_report_task = BashOperator(\n",
    "    task_id='generate_report',\n",
    "    bash_command='generate_report.sh',\n",
    "    start_date=datetime(2023,2,20),\n",
    "    dag=report_dag\n",
    ")\n",
    "\n",
    "precheck >> generate_report_task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c402386-4b35-43c6-8490-aac48bee40ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
