{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee846e1-9140-4fd0-a271-bfedcc6a3ba8",
   "metadata": {},
   "source": [
    "## Working with templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb98ef3-a96e-4651-b074-3f905ada3872",
   "metadata": {},
   "outputs": [],
   "source": [
    "templated_command=\"\"\"  \n",
    "echo \"Reading {{ params.filename }}\"\n",
    "\"\"\"\n",
    "t1 = BashOperator(task_id='template_task',   \n",
    "                  bash_command=templated_command,   \n",
    "                  params={'filename': 'file1.txt'},   \n",
    "                  dag=example_dag)\n",
    "\n",
    "t2 = BashOperator(task_id='template_task',  \n",
    "                  bash_command=templated_command,\n",
    "                  params={'filename': 'file2.txt'}, \n",
    "                  dag=example_dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60efe4ca-b1e1-4fef-bc4b-c55aac05533e",
   "metadata": {},
   "source": [
    "### Creating a templated BashOperator\n",
    "You've successfully created a BashOperator that cleans a given data file by executing a script called cleandata.sh. This works, but unfortunately requires the script to be run only for the current day. Some of your data sources are occasionally behind by a couple of days and need to be run manually.\n",
    "\n",
    "You successfully modify the cleandata.sh script to take one argument - the date in YYYYMMDD format. Your testing works at the command-line, but you now need to implement this into your Airflow DAG. For now, use the term {{ ds_nodash }} in your template - you'll see exactly what this means later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6cac8f-b452-4d90-9704-d42dd426ca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2023, 4, 15),\n",
    "}\n",
    "\n",
    "cleandata_dag = DAG('cleandata',\n",
    "                    default_args=default_args,\n",
    "                    schedule_interval='@daily')\n",
    "\n",
    "# Create a templated command to execute\n",
    "# 'bash cleandata.sh datestring'\n",
    "templated_command = \"\"\"\n",
    "bash cleandata.sh {{ ds_nodash }}\n",
    "\"\"\"\n",
    "\n",
    "# Modify clean_task to use the templated command\n",
    "clean_task = BashOperator(task_id='cleandata_task',\n",
    "                          bash_command=templated_command,\n",
    "                          dag=cleandata_dag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993d25c3-044d-4268-8854-1aaf7339bf02",
   "metadata": {},
   "source": [
    "### Templates with multiple arguments\n",
    "You wish to build upon your previous DAG and modify the code to support two arguments - the date in YYYYMMDD format, and a file name passed to the cleandata.sh script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a8af0a-2963-4882-bfa3-b8790e3ce7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2023, 4, 15),\n",
    "}\n",
    "\n",
    "cleandata_dag = DAG('cleandata',\n",
    "                    default_args=default_args,\n",
    "                    schedule_interval='@daily')\n",
    "\n",
    "# Modify the templated command to handle a\n",
    "# second argument called filename.\n",
    "templated_command = \"\"\"\n",
    "  bash cleandata.sh {{ ds_nodash }} {{ params.filename }}\n",
    "\"\"\"\n",
    "\n",
    "# Modify clean_task to pass the new argument\n",
    "clean_task = BashOperator(task_id='cleandata_task',\n",
    "                          bash_command=templated_command,\n",
    "                          params={'filename': 'salesdata.txt'},\n",
    "                          dag=cleandata_dag)\n",
    "\n",
    "# Create a new BashOperator clean_task2\n",
    "clean_task2 = BashOperator(task_id='cleandata_task2',\n",
    "                           bash_command=templated_command,\n",
    "                           params={'filename':'supportdata.txt'},\n",
    "                           dag=cleandata_dag)\n",
    "                           \n",
    "# Set the operator dependencies\n",
    "clean_task2 << clean_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace515c8-39e0-4dfa-b392-b954361bc050",
   "metadata": {},
   "source": [
    "## More templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb62d637-606b-4685-b298-e671e36ffe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More advanced template \n",
    "\n",
    "templated_command=\"\"\"\n",
    "{% for filename in params.filenames %} \n",
    "echo \"Reading {{ filename }}\"{% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "t1 = BashOperator(task_id='template_task',     \n",
    "                  bash_command=templated_command,  \n",
    "                  params={'filenames': ['file1.txt', 'file2.txt']}, \n",
    "                  dag=example_dag)\n",
    "\n",
    "Reading file1.txt\n",
    "Reading file2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1523dbef-3ecd-4a88-93c2-87d976e3773f",
   "metadata": {},
   "source": [
    "#### Variables \n",
    "- Airflow built-in runtime variables\n",
    "- Provides assorted information about DAG runs, tasks, and even the system configuration.\n",
    "- Examples include:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44d9015-f35a-41f5-921b-e8aaec89f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Execution Date: {{ ds }}                     # YYYY-MM-DD \n",
    "Execution Date, no dashes: {{ ds_nodash }}   # YYYYMMDD\n",
    "Previous Execution date: {{ prev_ds }}       # YYYY-MM-DD \n",
    "Prev Execution date, no dashes: {{ prev_ds_nodash }}  # YYYYMMDD\n",
    "DAG object: {{ dag }}\n",
    "Airflow config object: {{ conf }}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daa66ae-1418-4127-93f5-4f3833c5d2b9",
   "metadata": {},
   "source": [
    "### Using lists with templates\n",
    "Once again, you decide to make some modifications to the design of your cleandata workflow. This time, you realize that you need to run the command cleandata.sh with the date argument and the file argument as before, except now you have a list of 30 files. You do not want to create 30 tasks, so your job is to modify the code to support running the argument for 30 or more files.\n",
    "\n",
    "The Python list of files is already created for you, simply called filelist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574dbe5c-766b-480d-a8fb-f42a6523015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "filelist = [f'file{x}.txt' for x in range(30)]\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2020, 4, 15),\n",
    "}\n",
    "\n",
    "cleandata_dag = DAG('cleandata',\n",
    "                    default_args=default_args,\n",
    "                    schedule_interval='@daily')\n",
    "\n",
    "# Modify the template to handle multiple files in a \n",
    "# single run.\n",
    "templated_command = \"\"\"\n",
    "  <% for filename in params.filenames %>\n",
    "  bash cleandata.sh {{ ds_nodash }} {{ filename }};\n",
    "  <% endfor %>\n",
    "\"\"\"\n",
    "\n",
    "# Modify clean_task to use the templated command\n",
    "clean_task = BashOperator(task_id='cleandata_task',\n",
    "                          bash_command=templated_command,\n",
    "                          params={'filenames': filelist},\n",
    "                          dag=cleandata_dag)\n",
    "                          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15c54ed-e5ca-4fea-833c-31383248fa80",
   "metadata": {},
   "source": [
    "### Sending templated emails\n",
    "While reading through the Airflow documentation, you realize that various operations can use templated fields to provide added flexibility. You come across the docs for the EmailOperator and see that the content can be set to a template. You want to make use of this functionality to provide more detailed information regarding the output of a DAG run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c445d375-fac4-41d8-bec1-761038588306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.email import EmailOperator\n",
    "from datetime import datetime\n",
    "\n",
    "# Create the string representing the html email content\n",
    "html_email_str = \"\"\"\n",
    "Date: {{ ds }}\n",
    "Username: {{ params.username }}\n",
    "\"\"\"\n",
    "\n",
    "email_dag = DAG('template_email_test',\n",
    "                default_args={'start_date': datetime(2023, 4, 15)},\n",
    "                schedule_interval='@weekly')\n",
    "                \n",
    "email_task = EmailOperator(task_id='email_task',\n",
    "                           to='testuser@datacamp.com',\n",
    "                           subject=\"{{ macros.uuid.uuid4() }}\",\n",
    "                           html_content=html_email_str,\n",
    "                           params={'username': 'testemailuser'},\n",
    "                           dag=email_dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459dd6f5-9942-4cc9-af0a-e3c7ff023505",
   "metadata": {},
   "source": [
    "## Branching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4839301-2bfc-4581-beae-2404eb056262",
   "metadata": {},
   "source": [
    "Branching in Airflow: \n",
    "- Provides conditional logic\n",
    "- Using BranchPythonOperator from airflow.operators.python import BranchPythonOperator\n",
    "- Takes a python_callable to return the next task id (or list of ids) to follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcc3dad-c083-46cf-b875-8a4a4b806bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def branch_test(**kwargs):\n",
    "    if int(kwargs['ds_nodash']) % 2 == 0:\n",
    "        return'even_day_task'\n",
    "    else:\n",
    "        return'odd_day_task'\n",
    "\n",
    "branch_task = BranchPythonOperator(task_id='branch_task',dag=dag, \n",
    "                                   provide_context=True,   \n",
    "                                   python_callable=branch_test)\n",
    "\n",
    "start_task >> branch_task >> even_day_task >> even_day_task2 \n",
    "branch_task >> odd_day_task >> odd_day_task2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c036138-45fc-4f00-9ce9-8a5e7197a8db",
   "metadata": {},
   "source": [
    "### Define a BranchPythonOperator\n",
    "After learning about the power of conditional logic within Airflow, you wish to test out the BranchPythonOperator. You'd like to run a different code path if the current execution date represents a new year (ie, 2020 vs 2019).\n",
    "\n",
    "The DAG is defined for you, along with the tasks in question. Your current task is to implement the BranchPythonOperator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ad98b5-7770-4243-b9db-113a71108672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to determine if years are different\n",
    "def year_check(**kwargs):\n",
    "    current_year = int(kwargs['ds_nodash'][0:4])\n",
    "    previous_year = int(kwargs['prev_ds_nodash'][0:4])\n",
    "    if current_year == previous_year:\n",
    "        return 'current_year_task'\n",
    "    else:\n",
    "        return 'new_year_task'\n",
    "\n",
    "# Define the BranchPythonOperator\n",
    "branch_task = BranchPythonOperator(task_id='branch_task', dag=branch_dag,\n",
    "                                   python_callable=year_check, provide_context=True)\n",
    "# Define the dependencies\n",
    "branch_task >> current_year_task\n",
    "branch_task >> new_year_task "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b94100-69d1-4543-9aa5-87a4327873a7",
   "metadata": {},
   "source": [
    "## Creating a production pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8f53fa-e303-43e4-b4fd-67fb33b667eb",
   "metadata": {},
   "source": [
    "### Creating a production pipeline #1\n",
    "You've learned a lot about how Airflow works - now it's time to implement your workflow into a production pipeline consisting of many objects including sensors and operators. Your boss is interested in seeing this workflow become automated and able to provide SLA reporting as it provides some extra leverage for closing a deal the sales staff is working on. The sales prospect has indicated that once they see updates in an automated fashion, they're willing to sign-up for the indicated data service.\n",
    "\n",
    "From what you've learned about the process, you know that there is sales data that will be uploaded to the system. Once the data is uploaded, a new file should be created to kick off the full processing, but something isn't working correctly.\n",
    "\n",
    "Refer to the source code of the DAG to determine if anything extra needs to be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3702a4-1835-4cfe-a040-808acddd7571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "\n",
    "# Import the needed operators\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import date, datetime\n",
    "\n",
    "def process_data(**context):\n",
    "  file = open('/home/repl/workspace/processed_data.tmp', 'w')\n",
    "  file.write(f'Data processed on {date.today()}')\n",
    "  file.close()\n",
    "\n",
    "    \n",
    "dag = DAG(dag_id='etl_update', default_args={'start_date': datetime(2023,4,1)})\n",
    "\n",
    "sensor = FileSensor(task_id='sense_file', \n",
    "                    filepath='/home/repl/workspace/startprocess.txt',\n",
    "                    poke_interval=5,\n",
    "                    timeout=15,\n",
    "                    dag=dag)\n",
    "\n",
    "bash_task = BashOperator(task_id='cleanup_tempfiles', \n",
    "                         bash_command='rm -f /home/repl/*.tmp',\n",
    "                         dag=dag)\n",
    "\n",
    "python_task = PythonOperator(task_id='run_processing', \n",
    "                             python_callable=process_data,\n",
    "                             dag=dag)\n",
    "\n",
    "sensor >> bash_task >> python_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6a0a4-939f-45d5-bfb1-60ee688c7a0a",
   "metadata": {},
   "source": [
    "### Creating a production pipeline #2\n",
    "Continuing on your last workflow, you'd like to add some additional functionality, specifically adding some SLAs to the code and modifying the sensor components.\n",
    "\n",
    "Refer to the source code of the DAG to determine if anything extra needs to be added. The default_args dictionary has been defined for you, though it may require further modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f467d8-9482-48f5-906e-f42e329c4e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from dags.process import process_data\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "# Update the default arguments and apply them to the DAG\n",
    "default_args = {\n",
    "  'start_date': datetime(2023,1,1),\n",
    "  'sla': timedelta(minutes=90)\n",
    "}\n",
    "\n",
    "dag = DAG(dag_id='etl_update', default_args=default_args)\n",
    "\n",
    "sensor = FileSensor(task_id='sense_file', \n",
    "                    filepath='/home/repl/workspace/startprocess.txt',\n",
    "                    poke_interval= 45,\n",
    "                    dag=dag)\n",
    "\n",
    "bash_task = BashOperator(task_id='cleanup_tempfiles', \n",
    "                         bash_command='rm -f /home/repl/*.tmp',\n",
    "                         dag=dag)\n",
    "\n",
    "python_task = PythonOperator(task_id='run_processing', \n",
    "                             python_callable=process_data,\n",
    "                             provide_context= True,\n",
    "                             dag=dag)\n",
    "\n",
    "sensor >> bash_task >> python_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141699c4-847c-461c-8848-627f03f962b5",
   "metadata": {},
   "source": [
    "### Adding the final changes to your pipeline\n",
    "To finish up your workflow, your manager asks that you add a conditional logic check to send a sales report via email, only if the day is a weekday. Otherwise, no email should be sent. In addition, the email task should be templated to include the date and a project name in the content.\n",
    "\n",
    "The branch callable is already defined for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78af99ea-09dd-4bd7-aa90-f9e9444c6b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.python import BranchPythonOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from airflow.operators.email import EmailOperator\n",
    "from dags.process import process_data\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Update the default arguments and apply them to the DAG.\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2023,1,1),\n",
    "  'sla': timedelta(minutes=90)\n",
    "}\n",
    "    \n",
    "dag = DAG(dag_id='etl_update', default_args=default_args)\n",
    "\n",
    "sensor = FileSensor(task_id='sense_file', \n",
    "                    filepath='/home/repl/workspace/startprocess.txt',\n",
    "                    poke_interval=45,\n",
    "                    dag=dag)\n",
    "\n",
    "bash_task = BashOperator(task_id='cleanup_tempfiles', \n",
    "                         bash_command='rm -f /home/repl/*.tmp',\n",
    "                         dag=dag)\n",
    "\n",
    "python_task = PythonOperator(task_id='run_processing', \n",
    "                             python_callable=process_data,\n",
    "                             provide_context=True,\n",
    "                             dag=dag)\n",
    "\n",
    "email_subject=\"\"\"\n",
    "  Email report for {{ params.department }} on {{ ds_nodash }}\n",
    "\"\"\"\n",
    "\n",
    "email_report_task = EmailOperator(task_id='email_report_task',\n",
    "                                  to='sales@mycompany.com',\n",
    "                                  subject=email_subject,\n",
    "                                  html_content='',\n",
    "                                  params={email_subject: 'Data subscription services'},\n",
    "                                  dag=dag)\n",
    "\n",
    "no_email_task = EmptyOperator(task_id='no_email_task', dag=dag)\n",
    "\n",
    "def check_weekend(**kwargs):\n",
    "    dt = datetime.strptime(kwargs['execution_date'],\"%Y-%m-%d\")\n",
    "    # If dt.weekday() is 0-4, it's Monday - Friday. If 5 or 6, it's Sat / Sun.\n",
    "    if (dt.weekday() < 5):\n",
    "        return \"email_report_task\"\n",
    "    else:\n",
    "        return \"no_email_task\"\n",
    "    \n",
    "branch_task = BranchPythonOperator(task_id='check_if_weekend',\n",
    "                                   python_callable=check_weekend,\n",
    "                                   provide_context=True,                                \n",
    "                                   dag=dag)\n",
    "\n",
    "sensor >> bash_task >> python_task\n",
    "\n",
    "python_task >> branch_task >> [email_report_task, no_email_task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b2740d-2129-44a7-ba37-83478c44b942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
