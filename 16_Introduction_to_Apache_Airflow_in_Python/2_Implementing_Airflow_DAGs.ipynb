{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e7a9ae-c0fb-4f8f-86c8-577bfe16693e",
   "metadata": {},
   "source": [
    "## Airflow operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b886df-08b8-4515-9572-f490e3b41f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operators \n",
    "# Represent a single task in a workflow.\n",
    "# Run independently (usually).\n",
    "# Generally do not share information.\n",
    "# Various operators to perform different tasks.\n",
    "\n",
    "# New way, Airflow 2.x+ \n",
    "EmptyOperator(task_id='example')\n",
    "\n",
    "# Old way, Airflow <2.0\n",
    "EmptyOperator(task_id='example', dag=dag_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85d3ca5-95a9-46a2-b132-5edb7365b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BashOperator\n",
    "BashOperator(task_id='bash_example', \n",
    "             bash_command='echo \"Example!\"',\n",
    "             # Next line only for Airflow before v \n",
    "             dag=dag)\n",
    "\n",
    "BashOperator(task_id='bash_script_example', \n",
    "             bash_command='runcleanup.sh',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49fd918-84ee-45c8-a7ed-bb0678e7fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators.bash import BashOperator \n",
    "\n",
    "example_task = BashOperator(task_id='bash_ex',\n",
    "                            bash_command='echo 1',\n",
    "                           )\n",
    "\n",
    "bash_task = BashOperator(task_id='clean_addresses',  \n",
    "                         bash_command='cat addresses.txt | awk \"NF==10\" > cleaned.txt',)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4148a8b7-ea71-4ddc-9c00-48b4f2db34fe",
   "metadata": {},
   "source": [
    "### Defining a BashOperator task\n",
    "The BashOperator allows you to specify any given Shell command or script and add it to an Airflow workflow. This can be a great start to implementing Airflow in your environment.\n",
    "\n",
    "As such, you've been running some scripts manually to clean data (using a script called cleanup.sh) prior to delivery to your colleagues in the Data Analytics group. As you get more of these tasks assigned, you've realized it's becoming difficult to keep up with running everything manually, much less dealing with errors or retries. You'd like to implement a simple script as an Airflow operator.\n",
    "\n",
    "The Airflow DAG analytics_dag is already defined for you and has the appropriate configurations in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f620bdc-6e23-483a-b511-0d1c1f328599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the BashOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "with DAG(dag_id=\"test_dag\", default_args={\"start_date\": \"2024-01-01\"}) as analytics_dag:\n",
    "  # Define the BashOperator \n",
    "  cleanup = BashOperator(\n",
    "      task_id='cleanup_task',\n",
    "      # Define the bash_command\n",
    "      bash_command='cleanup.sh',\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033d8c88-4e8e-4d47-a701-f5b2092ed11d",
   "metadata": {},
   "source": [
    "### Multiple BashOperators\n",
    "Airflow DAGs can contain many operators, each performing their defined tasks.\n",
    "\n",
    "You've successfully implemented one of your scripts as an Airflow task and have decided to continue migrating your individual scripts to a full Airflow DAG. You now want to add more components to the workflow. In addition to the cleanup.sh used in the previous exercise you have two more scripts, consolidate_data.sh and push_data.sh. These further process your data and copy to its final location.\n",
    "\n",
    "The DAG analytics_dag is defined (meaning you do not need to add the with DAG(...) statement, and your cleanup task is still defined. The BashOperator is already imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195b00be-e3b9-45d1-8758-e978015c2806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a second operator to run the `consolidate_data.sh` script\n",
    "consolidate = BashOperator(\n",
    "    task_id='consolidate_task',\n",
    "    bash_command='consolidate_data.sh'\n",
    "    )\n",
    "\n",
    "# Define a final operator to execute the `push_data.sh` script\n",
    "push_data = BashOperator(\n",
    "    task_id='pushdata_task',\n",
    "    bash_command='push_data.sh'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ee6209-0378-4859-9951-784d96c85a04",
   "metadata": {},
   "source": [
    "## Airflow tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1be17ba-6057-4e99-949a-8d22a12a5cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_task = BashOperator(task_id='bash_example',       \n",
    "                            bash_command='echo \"Example!\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32080489-5ebe-411a-b5d2-eb4e565dbed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Upstream means before\n",
    "Downstream means after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe521a2f-c697-4449-93f1-a5162c0082aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tasks \n",
    "task1 = BashOperator(task_id='first_task',     \n",
    "                     bash_command='echo 1'         \n",
    "                    )\n",
    "\n",
    "task2 = BashOperator(task_id='second_task', \n",
    "                     bash_command='echo 2'    \n",
    "                    ) \n",
    "\n",
    "# Set first_task to run before second_task\n",
    "task1 >> task2   # or task2 << task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00178740-bcc1-4deb-a19a-6fb5787e023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple dependencies \n",
    "# Chained dependencies: \n",
    "task1 >> task2 >> task3 >> task4 \n",
    "\n",
    "# Mixed dependencies:\n",
    "task1 >> task2 << task3\n",
    "\n",
    "# or:\n",
    "task1 >> task2\n",
    "task3 >> task2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facadb96-0976-494d-aa95-9bd931cd2928",
   "metadata": {},
   "source": [
    "### Define order of BashOperators\n",
    "Now that you've learned about the bitshift operators, it's time to modify your workflow to include a pull step and to include the task ordering. You have three currently defined components, cleanup, consolidate, and push_data.\n",
    "\n",
    "The DAG analytics_dag is available as before and the BashOperator is already imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d40025-676c-4fb2-a523-a4b229dca715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new pull_sales task\n",
    "pull_sales = BashOperator(\n",
    "    task_id='pullsales_task',\n",
    "    bash_command='wget https://salestracking/latestinfo?json'\n",
    ")\n",
    "\n",
    "# Set pull_sales to run prior to cleanup\n",
    "pull_sales >> cleanup \n",
    "\n",
    "# Configure consolidate to run after cleanup\n",
    "cleanup >> consolidate \n",
    "\n",
    "# Set push_data to run last\n",
    "consolidate >> push_data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c1a49-59e9-42b9-91c0-47254e2dc123",
   "metadata": {},
   "source": [
    "## Additional operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c546d91c-37d5-4cfa-8f52-4b92efe111d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PythonOperator\n",
    "from airflow.operators.python import PythonOperator \n",
    "\n",
    "def printme():\n",
    "    print(\"This goes in the logs!\")\n",
    "python_task = PythonOperator(\n",
    "    task_id='simple_print',\n",
    "    python_callable=printme\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4817be5-fb6f-4cc0-9085-53ea6e0f2c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# op_kwargs example \n",
    "def sleep(length_of_time):  \n",
    "    time.sleep(length_of_time)\n",
    "    sleep_task = PythonOperator( \n",
    "        task_id='sleep',  \n",
    "        python_callable=sleep,  \n",
    "        op_kwargs={'length_of_time': 5}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41c3f5d-72d3-4d0e-9db1-f5e02255e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Email Operator example \n",
    "from airflow.operators.email import EmailOperator \n",
    "\n",
    "email_task = EmailOperator(   \n",
    "    task_id='email_sales_report', \n",
    "    to='sales_manager@example.com',  \n",
    "    subject='Automated Sales Report',\n",
    "    html_content='Attached is the latest sales report',   \n",
    "    files='latest_sales.xlsx'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb5e192-9f97-4d77-b621-70208b3ea4b6",
   "metadata": {},
   "source": [
    "### Using the PythonOperator\n",
    "You've implemented several Airflow tasks using the BashOperator but realize that a couple of specific tasks would be better implemented using Python. You'll implement a task to download and save a file to the system within Airflow.\n",
    "\n",
    "The requests library is imported for you, and the DAG process_sales_dag is already defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bac86b-c9bc-4217-9f6f-72abc2357330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the method\n",
    "def pull_file(URL, savepath):\n",
    "    r = requests.get(URL)\n",
    "    with open(savepath, 'wb') as f:\n",
    "        f.write(r.content)    \n",
    "    # Use the print method for logging\n",
    "print(f\"File pulled from {'URL'} and saved to {'savepath'}\")\n",
    "\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "# Create the task\n",
    "pull_file_task = PythonOperator(\n",
    "    task_id='pull_file',\n",
    "    # Add the callable\n",
    "    python_callable=pull_file,\n",
    "    # Define the arguments\n",
    "    op_kwargs={'URL':'http://dataserver/sales.json', 'savepath':'latestsales.json'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98413087-91b9-4d60-9ae2-6e3a9b6094c7",
   "metadata": {},
   "source": [
    "### More PythonOperators\n",
    "To continue implementing your workflow, you need to add another step to parse and save the changes of the downloaded file. The DAG process_sales_dag is defined and has the pull_file task already added. In this case, the Python function is already defined for you, parse_file(inputfile, outputfile).\n",
    "\n",
    "Note that often when implementing Airflow tasks, you won't necessarily understand the individual steps given to you. As long as you understand how to wrap the steps within Airflow's structure, you'll be able to implement a desired workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecce34c1-accc-4217-b965-48a37b095d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add another Python task\n",
    "parse_file_task = PythonOperator(\n",
    "    task_id='parse_file',\n",
    "    # Set the function to call\n",
    "    python_callable=parse_file,\n",
    "    # Add the arguments\n",
    "    op_kwargs={'inputfile':'latestsales.json','outputfile':'parsedfile.json'},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eca07a-25c8-40ca-8170-e2f86b714334",
   "metadata": {},
   "source": [
    "### EmailOperator and dependencies\n",
    "Now that you've successfully defined the PythonOperators for your workflow, your manager would like to receive a copy of the parsed JSON file via email when the workflow completes. The previous tasks are still defined and the DAG process_sales_dag is configured. Please note that this task uses the older DAG definition method and is added for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0802e92-f8f2-4390-b8fb-01ef063c539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Operator\n",
    "from airflow.operators.email import EmailOperator\n",
    "\n",
    "# Define the task\n",
    "email_manager_task = EmailOperator(\n",
    "    task_id='email_manager',\n",
    "    to='manager@datacamp.com',\n",
    "    subject='Latest sales JSON',\n",
    "    html_content='Attached is the latest sales JSON file as requested.',\n",
    "    files='parsedfile.json',\n",
    "    dag=process_sales_dag\n",
    ")\n",
    "\n",
    "# Set the order of tasks\n",
    "pull_file_task >> parse_file_task >> email_manager_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465ab12e-563b-46a7-b777-d3456ec4c427",
   "metadata": {},
   "source": [
    "## Airflow scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122a61a1-33be-49ae-ab28-2e55b4bb833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Airflow scheduler presets \n",
    "\n",
    "Preset:    cron equivalent:\n",
    "@hourly.   0 * * * *\n",
    "@daily.    0 0 * * *\n",
    "@weekly    0 0 * * 0\n",
    "@monthly   0 0 1 * *\n",
    "@yearly    0 0 1 1 *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07751b63-6ce4-415a-b905-6e0b321d84c2",
   "metadata": {},
   "source": [
    "### Schedule a DAG via Python\n",
    "You've learned quite a bit about creating DAGs, but now you would like to schedule a specific DAG on a specific day of the week at a certain time. You'd like the code include this information in case a colleague needs to reinstall the DAG to a different server.\n",
    "\n",
    "The Airflow DAG object and the appropriate datetime methods have been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ac6e45-aef6-43c5-bc24-936afe9715ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the scheduling arguments as defined\n",
    "default_args = {\n",
    "  'owner': 'Engineering',\n",
    "  'start_date': datetime(2023,11,1),\n",
    "  'email': ['airflowresults@datacamp.com'],\n",
    "  'email_on_failure': False,\n",
    "  'email_on_retry': False,\n",
    "  'retries': 3,\n",
    "  'retry_delay': timedelta(minutes=20)\n",
    "}\n",
    "\n",
    "dag = DAG('update_dataflows', default_args=default_args, schedule_interval='30 12 * * 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9fc4aa-ee48-4ff9-b356-2f4a86403d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
