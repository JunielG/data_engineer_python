{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "928f4473-264c-4588-83c5-6bf9e983650c",
   "metadata": {},
   "source": [
    "### Project Description\n",
    "Mastering data pipelines is essential for Data Engineers today. It involves extracting, transforming, and loading data â€” a fundamental task that ensures information flows smoothly.\n",
    "In this project, you will work with retail data from a multinational retail corporation Walmart. You will retrieve data from different sources, like SQL and parquet; prepare the data using some transformation techniques, and finally load the data in an easy-to-access format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c61c9a5-58f8-4beb-9a5d-c8f4115519aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste the following SQL query into the SQL code cell without the leading # symbol:\n",
    "SELECT * FROM grocery_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f5f7c7-12aa-4008-b941-3a6b84d2428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste the following Python code into the Python code cell:\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# extract functin is already implemented for you \n",
    "def extract(store_data, extra_data):\n",
    "    extra_df = pd.read_parquet(extra_data)\n",
    "    merged_df = store_data.merge(extra_df, on = \"index\")\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1d48cc-bffa-4f83-900e-0cbdcb0c0548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the extract() function and store it as the \"merged_df\" variable\n",
    "merged_df = extract(grocery_sales, \"extra_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773b8807-46e8-4ce1-b7f9-dd53eb912a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transform() function with one parameter: \"raw_data\"\n",
    "def transform(raw_data):\n",
    "  # Fill NaNs using mean since we are dealing with numeric columns\n",
    "  # Set inplace = True to do the replacing on the current DataFrame\n",
    "    raw_data.fillna(\n",
    "      {\n",
    "          'CPI': raw_data['CPI'].mean(),\n",
    "          'Weekly_Sales': raw_data['Weekly_Sales'].mean(),\n",
    "          'Unemployment': raw_data['Unemployment'].mean(),\n",
    "      }, inplace = True\n",
    "    )\n",
    "\n",
    "    # Define the type of the \"Date\" column and its format\n",
    "    raw_data[\"Date\"] = pd.to_datetime(raw_data[\"Date\"], format = \"%Y-%m-%d\")\n",
    "    # Extract the month value from the \"Date\" column to calculate monthly sales later on\n",
    "    raw_data[\"Month\"] = raw_data[\"Date\"].dt.month\n",
    "\n",
    "    # Filter the entire DataFrame using the \"Weekly_Sales\" column. Use .loc to access a group of rows\n",
    "    raw_data = raw_data.loc[raw_data[\"Weekly_Sales\"] > 10000, :]\n",
    "    \n",
    "    # Drop unnecessary columns. Set axis = 1 to specify that the columns should be removed\n",
    "    raw_data = raw_data.drop([\"index\", \"Temperature\", \"Fuel_Price\", \"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\", \"Type\", \"Size\", \"Date\"], axis = 1)\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2548bad9-e9d1-497d-96d8-b18049093b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the transform() function and pass the merged DataFrame\n",
    "clean_data = transform(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b530a835-b9b7-4109-a83c-2134de29502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the avg_weekly_sales_per_month function that takes in the cleaned data from the last step\n",
    "def avg_weekly_sales_per_month(clean_data):\n",
    "  \t# Select the \"Month\" and \"Weekly_Sales\" columns as they are the only ones needed for this analysis\n",
    "    holidays_sales = clean_data[[\"Month\", \"Weekly_Sales\"]]\n",
    "   \t# Create a chain operation with groupby(), agg(), reset_index(), and round() functions\n",
    "    # Group by the \"Month\" column and calculate the average monthly sales\n",
    "    # Call reset_index() to start a new index order\n",
    "    # Round the results to two decimal places\n",
    "    \n",
    "    holidays_sales = (holidays_sales.groupby(\"Month\")\n",
    "    .agg(Avg_Sales = (\"Weekly_Sales\", \"mean\"))\n",
    "    .reset_index().round(2))\n",
    "    return holidays_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94296b3-b99a-4872-bb22-a8670a789f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the avg_weekly_sales_per_month() function and pass the cleaned DataFrame\n",
    "agg_data = avg_weekly_sales_per_month(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40026f5-1cfa-4f09-8391-0fe14d5ad07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the load() function that takes in the cleaned DataFrame and the aggregated one with the paths where they are going to be stored\n",
    "def load(full_data, full_data_file_path, agg_data, agg_data_file_path):\n",
    "  \t# Save both DataFrames as csv files. Set index = False to drop the index columns\n",
    "    full_data.to_csv(full_data_file_path, index = False)\n",
    "    agg_data.to_csv(agg_data_file_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a07130b-08f1-4f64-a19e-634eac3ee877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the load() function and pass the cleaned and aggregated DataFrames with their paths    \n",
    "load(clean_data, \"clean_data.csv\", agg_data, \"agg_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05428494-ff53-4062-8db8-301ee852bd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the validation() function with one parameter: file_path - to check whether the previous function was correctly executed\n",
    "def validation(file_path):\n",
    "  \t# Use the \"os\" package to check whether a path exists\n",
    "    file_exists = os.path.exists(file_path)\n",
    "    # Raise an exception if the path doesn't exist, hence, if there is no file found on a given path\n",
    "    if not file_exists:\n",
    "        raise Exception(f\"There is no file at the path {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9c6df4-56fd-4278-8c35-af959badb5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the validation() function and pass first, the cleaned DataFrame path, and then the aggregated DataFrame path\n",
    "validation(\"clean_data.csv\")\n",
    "validation(\"agg_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9ccba4-68eb-4658-b473-6c4bd707fa95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
