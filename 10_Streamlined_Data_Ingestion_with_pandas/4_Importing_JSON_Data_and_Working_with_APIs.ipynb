{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "329409b9-4395-470a-883e-68761ec7258b",
   "metadata": {},
   "source": [
    "## Introduction to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a818b08c-c286-43ae-afa9-7038a30270e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "death_causes = pd.read_json(\"nyc_death_causes.json\",\n",
    "                            orient=\"split\")\n",
    "death_causes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57cb3c5-d090-42c5-a932-2ce28d4d8ab4",
   "metadata": {},
   "source": [
    "### Load JSON data\n",
    "Many open data portals make available JSONs datasets that are particularly easy to parse. They can be accessed directly via URL. Each object is a record, all objects have the same set of attributes, and none of the values are nested objects that themselves need to be parsed.\n",
    "\n",
    "The New York City Department of Homeless Services Daily Report is such a dataset, containing years' worth of homeless shelter population counts. You can view it in the console before loading it to a dataframe with pandas's read_json() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe00497-5a22-428a-adbc-d8a4d30c86cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Load the daily report to a dataframe\n",
    "pop_in_shelters = pd.read_json(\"dhs_daily_report.json\")\n",
    "                            \n",
    "# View summary stats about pop_in_shelters\n",
    "print(pop_in_shelters.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed7280e-9c43-4686-858d-461a4398a4ba",
   "metadata": {},
   "source": [
    "### Work with JSON orientations\n",
    "JSON isn't a tabular format, so pandas makes assumptions about its orientation when loading data. Most JSON data you encounter will be in orientations that pandas can automatically transform into a dataframe.\n",
    "\n",
    "Sometimes, like in this modified version of the Department of Homeless Services Daily Report, data is oriented differently. To reduce the file size, it has been split formatted. You'll see what happens when you try to load it normally versus with the orient keyword argument. The try/except block will alert you if there are errors loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa29a745-9da4-47e9-bea5-7db4ab11095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the JSON without keyword arguments\n",
    "    df = pd.read_json('dhs_report_reformatted.json')\n",
    "    \n",
    "    # Plot total population in shelters over time\n",
    "    df[\"date_of_census\"] = pd.to_datetime(df[\"date_of_census\"])\n",
    "    df.plot(x=\"date_of_census\", \n",
    "            y=\"total_individuals_in_shelter\")\n",
    "    plt.show()\n",
    "    \n",
    "except ValueError:\n",
    "    print(\"pandas could not parse the JSON.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67a7b7b-cba0-441b-b3e7-e8688dd714cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the JSON with orient specified\n",
    "    df = pd.read_json(\"dhs_report_reformatted.json\",\n",
    "                      orient='split')\n",
    "    \n",
    "    # Plot total population in shelters over time\n",
    "    df[\"date_of_census\"] = pd.to_datetime(df[\"date_of_census\"])\n",
    "    df.plot(x=\"date_of_census\", \n",
    "            y=\"total_individuals_in_shelter\")\n",
    "    plt.show()\n",
    "    \n",
    "except ValueError:\n",
    "    print(\"pandas could not parse the JSON.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bafc817-8c87-4840-b807-e1f8c4015c98",
   "metadata": {},
   "source": [
    "## Introduction to APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90141cf-536d-4895-aa7d-4ee56f24833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "api_url = \"https://api.yelp.com/v3/businesses/search\"\n",
    "\n",
    "# Set up parameter dictionary according to documentation\n",
    "params = {\"term\": \"bookstore\", \"location\": \"San Francisco\"}\n",
    "\n",
    "# Set up header dictionary w/ API key according to documentation\n",
    "headers = {\"Authorization\": \"Bearer {}\".format(api_key)}\n",
    "\n",
    "# Call the API\n",
    "response = requests.get(api_url, params=params, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb95ebf-fb03-43d4-98ec-0e66d9e81145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the JSON data from the response object\n",
    "data = response.json()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb60f19f-5d4e-45a6-807b-8285139acf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load businesses data to a dataframe\n",
    "bookstores = pd.DataFrame(data[\"businesses\"])\n",
    "print(bookstores.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e863f28a-e182-4e63-8887-f07737bbb52e",
   "metadata": {},
   "source": [
    "### Get data from an API\n",
    "In this exercise, you'll use requests.get() to query the Yelp Business Search API for cafes in New York City. requests.get() needs a URL to get data from. The Yelp API also needs search parameters and authorization headers passed to the params and headers keyword arguments, respectively.\n",
    "\n",
    "You'll need to extract the data from the response with its json() method, and pass it to pandas's DataFrame() function to make a dataframe. Note that the necessary data is under the dictionary key \"businesses\".\n",
    "\n",
    "pandas (as pd) and requests have been loaded. Authorization data is in the dictionary headers, and the needed API parameters are stored as params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099c6113-a10c-494a-8a8d-f22300beae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = \"https://api.yelp.com/v3/businesses/search\"\n",
    "\n",
    "# Get data about NYC cafes from the Yelp API\n",
    "response = requests.get(api_url, \n",
    "                headers=headers, \n",
    "                params=params)\n",
    "\n",
    "# Extract JSON data from the response\n",
    "data = response.json()\n",
    "\n",
    "# Load data to a dataframe\n",
    "cafes = pd.DataFrame(data['businesses'])\n",
    "\n",
    "# View the data's dtypes\n",
    "print(cafes.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b6c57b-edbc-4578-ba3b-6837f97ec256",
   "metadata": {},
   "source": [
    "### Set API parameters\n",
    "Formatting parameters to get the data you need is an integral part of working with APIs. These parameters can be passed to the get() function's params keyword argument as a dictionary.\n",
    "\n",
    "The Yelp API requires the location parameter be set. It also lets users supply a term to search for. You'll use these parameters to get data about cafes in NYC, then process the result to create a dataframe.\n",
    "\n",
    "pandas (as pd) and requests have been loaded. The API endpoint is stored in the variable api_url. Authorization data is stored in the dictionary headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd742cef-06ad-4ec6-89fe-05ff57722904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary to query API for cafes in NYC\n",
    "parameters = {'term':'cafe',\n",
    "          \t  'location':'NYC'}\n",
    "\n",
    "# Query the Yelp API with headers and params set\n",
    "response = requests.get(api_url,\n",
    "                headers=headers,\n",
    "                params=parameters)\n",
    "\n",
    "# Extract JSON data from response\n",
    "data = response.json()\n",
    "\n",
    "# Load \"businesses\" values to a dataframe and print head\n",
    "cafes = pd.DataFrame(data['businesses'])\n",
    "print(cafes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68384d0a-3d44-471a-8135-216e2cf85ae3",
   "metadata": {},
   "source": [
    "### Set request headers\n",
    "Many APIs require users provide an API key, obtained by registering for the service. Keys typically are passed in the request header, rather than as parameters.\n",
    "\n",
    "The Yelp API documentation says \"To authenticate API calls with the API Key, set the Authorization HTTP header value as Bearer api_key.\"\n",
    "\n",
    "You'll set up a dictionary to pass this information to get(), call the API for the highest-rated cafes in NYC, and parse the response.\n",
    "\n",
    "pandas (as pd) and requests have been loaded. The API endpoint is stored as api_url, and the key is api_key. Parameters are in the dictionary params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ab1510-8c31-42d0-9963-0bc196135ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary that passes Authorization and key string\n",
    "headers = {'Authorization': \"Bearer {}\".format(api_key)}\n",
    "\n",
    "# Query the Yelp API with headers and params set\n",
    "response = requests.get(api_url, headers=headers,params=params)\n",
    "\n",
    "\n",
    "\n",
    "# Extract JSON data from response\n",
    "data = response.json()\n",
    "\n",
    "# Load \"businesses\" values to a dataframe and print names\n",
    "cafes = pd.DataFrame(data['businesses'])\n",
    "print(cafes.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b9399a-e8de-441b-a8b9-8de8e7aad4d0",
   "metadata": {},
   "source": [
    "## Working with nested JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e0ed4d-b412-4cdf-a32d-7a3e08f976b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# Set up headers, parameters, and API endpoint\n",
    "api_url = \"https://api.yelp.com/v3/businesses/search\"\n",
    "headers = {\"Authorization\": \"Bearer {}\".format(api_key)}\n",
    "params = {\"term\": \"bookstore\", \"location\": \"San Francisco\"}\n",
    "\n",
    "# Make the API call and extract the JSON data\n",
    "response = requests.get(api_url, headers=headers, params=params)\n",
    "data = response.json()\n",
    "\n",
    "# Flatten data and load to dataframe, with _ separators\n",
    "bookstores = json_normalize(data[\"businesses\"], sep=\"_\")\n",
    "print(list(bookstores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0984e9-919d-452f-a86e-53caa22beb38",
   "metadata": {},
   "source": [
    "### Flatten nested JSONs\n",
    "A feature of JSON data is that it can be nested: an attribute's value can consist of attribute-value pairs. This nested data is more useful unpacked, or flattened, into its own dataframe columns. The pandas.io.json submodule has a function, json_normalize(), that does exactly this.\n",
    "\n",
    "The Yelp API response data is nested. Your job is to flatten out the next level of data in the coordinates and location columns.\n",
    "\n",
    "pandas (as pd) and requests have been imported. The results of the API call are stored as response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12086f91-d73e-406c-a3ed-0b58bdf606fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load json_normalize()\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# Isolate the JSON data from the API response\n",
    "data = response.json()\n",
    "\n",
    "# Flatten business data into a dataframe, replace separator\n",
    "cafes = json_normalize(data[\"businesses\"],\n",
    "             sep='_')\n",
    "\n",
    "# View data\n",
    "print(cafes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3bf65b-4330-4fd9-9cef-8ced9158c4b9",
   "metadata": {},
   "source": [
    "### Handle deeply nested data\n",
    "Last exercise, you flattened data nested down one level. Here, you'll unpack more deeply nested data.\n",
    "\n",
    "The categories attribute in the Yelp API response contains lists of objects. To flatten this data, you'll employ json_normalize() arguments to specify the path to categories and pick other attributes to include in the dataframe. You should also change the separator to facilitate column selection and prefix the other attributes to prevent column name collisions. We'll work through this in steps.\n",
    "\n",
    "pandas (as pd) and json_normalize() have been imported. JSON-formatted Yelp data on cafes in NYC is stored as data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640e4a00-198a-4a0e-b5e2-e1b4b8ea20f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load other business attributes and set meta prefix\n",
    "flat_cafes = json_normalize(data[\"businesses\"],\n",
    "                            sep=\"_\",\n",
    "                    \t\trecord_path=\"categories\",\n",
    "                    \t\tmeta=['name', \n",
    "                                  'alias',  \n",
    "                                  'rating',\n",
    "                          \t\t  [\"coordinates\", \"latitude\"],    \n",
    "                                  [\"coordinates\", \"longitude\"]],\n",
    "                    \t\tmeta_prefix=\"biz_\")\n",
    "\n",
    "# View the data\n",
    "print(flat_cafes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5e5c55-0772-4dcf-89fe-3f9e8dd0f985",
   "metadata": {},
   "source": [
    "## Combining multiple datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da24b3a-6f9a-4774-8273-651b5cf9d701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first 20 bookstore results\n",
    "params = {\"term\": \"bookstore\", \"location\": \"San Francisco\"}  \n",
    "first_results = requests.get(api_url,            \n",
    "                             headers=headers,       \n",
    "                             params=params).json()\n",
    "\n",
    "first_20_bookstores = json_normalize(first_results[\"businesses\"],\n",
    "                                     sep=\"_\")\n",
    "\n",
    "print(first_20_bookstores.shape)\n",
    "\n",
    "# Get the next 20 bookstores\n",
    "params[\"offset\"] = 20\n",
    "next_results = requests.get(api_url, \n",
    "                            headers=headers, \n",
    "                            params=params).json()\n",
    "\n",
    "next_20_bookstores = json_normalize(next_results[\"businesses\"], sep=\"_\")\n",
    "\n",
    "print(next_20_bookstores.shape)\n",
    "\n",
    "\n",
    "# Put bookstore datasets together, renumber rows\n",
    "bookstores = pd.concat([first_20_bookstores, next_20_bookstores],\n",
    "                       ignore_index=True)\n",
    "\n",
    "print(bookstores.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1aca84-4c02-4c71-a5bf-d1d0d941ca74",
   "metadata": {},
   "source": [
    "### Concatenate dataframes\n",
    "In this exercise, you’ll practice concatenating records by creating a dataset of the 100 highest-rated cafes in New York City according to Yelp.\n",
    "\n",
    "APIs often limit the amount of data returned, since sending large datasets can be time- and resource-intensive. The Yelp Business Search API limits the results returned in a call to 50 records. However, the offset parameter lets a user retrieve results starting after a specified number. By modifying the offset, we can get results 1-50 in one call and 51-100 in another. Then, we can append the dataframes.\n",
    "\n",
    "pandas (as pd), requests, and json_normalize() have been imported. The 50 top-rated cafes are already in a dataframe, top_50_cafes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346a240b-fb26-4712-9c37-82a347973570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an offset parameter to get cafes 51-100\n",
    "params = {\"term\": \"cafe\", \n",
    "          \"location\": \"NYC\",\n",
    "          \"sort_by\": \"rating\", \n",
    "          \"limit\": 50,\n",
    "          \"offset\": 50}\n",
    "\n",
    "result = requests.get(api_url, headers=headers, params=params)\n",
    "next_50_cafes = json_normalize(result.json()[\"businesses\"])\n",
    "\n",
    "# Concatenate the results, setting ignore_index to renumber rows\n",
    "cafes = pd.concat([top_50_cafes, next_50_cafes], ignore_index=True)\n",
    "\n",
    "# Print shape of cafes\n",
    "print(cafes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88741ec-7645-4e70-aae1-c4acd2e32511",
   "metadata": {},
   "source": [
    "### Merge dataframes\n",
    "In the last exercise, you built a dataset of the top 100 cafes in New York City according to Yelp. Now, you'll combine that with demographic data to investigate which neighborhood has the most good cafes per capita.\n",
    "\n",
    "To do this, you'll merge two datasets with the DataFrame merge() method. The first,crosswalk, is a crosswalk between ZIP codes and Public Use Micro Data Sample Areas (PUMAs), which are aggregates of census tracts and correspond roughly to NYC neighborhoods. Then, you'll merge in pop_data, which contains 2016 population estimates for each PUMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f462c4e5-f528-49b5-b784-7c89e33a6c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge crosswalk into cafes on their zip code fields\n",
    "cafes_with_pumas = pd.merge(cafes, crosswalk, \n",
    "left_on='location_zip_code', right_on= 'zipcode')\n",
    "\n",
    "\n",
    "\n",
    "# Merge pop_data into cafes_with_pumas on puma field\n",
    "cafes_with_pop = pd.merge(cafes_with_pumas, pop_data, on='puma')\n",
    "\n",
    "# View the data\n",
    "print(cafes_with_pop.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6884edfc-fde8-4695-8c5d-9b8fdcf4507f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
